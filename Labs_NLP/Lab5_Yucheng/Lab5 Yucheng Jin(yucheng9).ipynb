{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5: Machine Translation\n",
    "# Yucheng Jin (yucheng9)\n",
    "\n",
    "Machine translation is an application of NLP to computationally translate text from one language to the other. It is one of the most popular fields of research in the NLP community. Very recently (2014) deep learning methods changed the face of machine translation and all of NLP. The application of deep learning to machine translation was referred to as Neural Machine Translation (NMT).\n",
    "\n",
    "In this mini lab, we will see a sample code that performs neural machine translation, and we will see the effects of various hyperparameters on the performance of the system.\n",
    "\n",
    "For simplicity, we will take as our dataset, a set of short English sentences mapped to French sentences, and instead of performing the translation one word at a time, it will be performing it one character at a time.\n",
    "\n",
    "Total points: 20 points + 10 bonus points\n",
    "\n",
    "**Submission Instructions**: Just upload this notebook, with all your answers in the respective cells, to Compass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code contains the full code of the functional NMT system. Run it to get a sense of how the loss and accuracy of the training data and the validation data are changing with every epoch. No data is necessary to download; all is included in the lab's directory.\n",
    "\n",
    "#### Packages to install:\n",
    "- pip install keras\n",
    "- pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 1.2368 - accuracy: 0.7221 - val_loss: 1.1127 - val_accuracy: 0.6966\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.9124 - accuracy: 0.7481 - val_loss: 0.9669 - val_accuracy: 0.7341\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.7940 - accuracy: 0.7838 - val_loss: 0.8391 - val_accuracy: 0.7688\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.6874 - accuracy: 0.8065 - val_loss: 0.7541 - val_accuracy: 0.7847\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 8s 996us/step - loss: 0.6226 - accuracy: 0.8210 - val_loss: 0.6956 - val_accuracy: 0.8009\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.5837 - accuracy: 0.8305 - val_loss: 0.6580 - val_accuracy: 0.8084\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.5565 - accuracy: 0.8379 - val_loss: 0.6374 - val_accuracy: 0.8132\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.5351 - accuracy: 0.8437 - val_loss: 0.6197 - val_accuracy: 0.8174\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.5166 - accuracy: 0.8488 - val_loss: 0.6026 - val_accuracy: 0.8240\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.5004 - accuracy: 0.8532 - val_loss: 0.5843 - val_accuracy: 0.8278\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Sois de moit !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Sois de mont !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Sois de mont !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Atrez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Atrez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui le te trai ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Sous de mous !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: AtrÃªtez de moit !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Sois de moit !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Atrez de cherte !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Atrez de cherte !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Atrez de cherte !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Atres-le !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Atres-le !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je suis pas de main.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Il est paste.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je suis pas de main.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je suis pas de main.\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: Je suis pas de main.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Pentez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Artez de cherte !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Artez de cherte !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Sois de moit !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Sois de moit !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Sois de moit !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Sois de moit !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Atrez de cherte !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Atrez de cherte !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Atrez de cherte !\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Prendez pas !\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Prendez pas !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Prendez pas !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Prendez pas !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je me suis pas de main.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je me suis pas de main.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: J'ai le paste.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je me suis pas de main.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je me suis pas de main.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: J'ai partie de main.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: Je l'ai pas de main.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Je suis pas de main.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis pas de main.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis pas de main.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Artrez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Sous de me mait !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Sous de me mait !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Sous de me mait !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Sous de me mait !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Sous de me mait !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Sous de me mait !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Sous de me mait !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Sous de me mait !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Sous de me mait !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Atrez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Atrez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Atrez-vous !\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Prendez pas !\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: Nous sommes pas pas !\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous sommes pas de mais.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous sommes pas de mais.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous sommes pas de mais.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous sommes pas de mais.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Sois de moit !\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Sois de mous !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sois de moit !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sois de moit !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sois de moit !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Sois de mous !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois sont pas !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois sont pas !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois sont pas !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois sont pas !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois sont pas !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois sont pas !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Sois de moit !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois sonte !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois sonte !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois sonte !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois sonte !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois sonte !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois sonte !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Artez de mait.\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Lais-le !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Lais-le !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Sequence to sequence example in Keras (character-level).\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "**Summary of the algorithm**\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and corresponding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    It uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "**Data download**\n",
    "[English to French sentence pairs.\n",
    "](http://www.manythings.org/anki/fra-eng.zip)\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "latent_dim = 100  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training an NMT system (or any deep learning system), one of the decisions to take is the batch size: number of examples processed (in parallel) before updating parameters.\n",
    "\n",
    "**Deliverable 1** (5 points): Change the batch size from 64 to 128, and then 256. How did it affect the speed of training (time per epoch)? Is it slower or faster? Why? What was its effect on the increase of accuracy from one epoch to the other? What is the concluded tradeoff between a smaller batch size and a larger one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Per Epoch of Different Batch Sizes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>average time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>batch size = 64</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch size = 128</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch size = 256</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1  2  3   4  5  6  7  8  9  10  average time (s)\n",
       "batch size = 64   9  9  9  10  9  8  9  8  8   8               8.7\n",
       "batch size = 128  8  9  8   9  9  9  9  8  8   8               8.5\n",
       "batch size = 256  8  8  8   8  8  8  8  8  8   8               8.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data here, code below\n",
    "import statistics\n",
    "from statistics import mean\n",
    "import pandas as pd\n",
    "b_64 = [9, 9, 9, 10, 9, 8, 9, 8, 8, 8]\n",
    "b_128 = [8, 9, 8, 9, 9, 9, 9, 8, 8, 8]\n",
    "b_256 = [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
    "options_list = [\"batch size = 64\", \"batch size = 128\", \"batch size = 256\"]\n",
    "b_64.append(mean(b_64))\n",
    "b_128.append(mean(b_128))\n",
    "b_256.append(mean(b_256))\n",
    "data = [b_64, b_128, b_256] \n",
    "title = list(range(1,11))\n",
    "title.append('average time (s)')\n",
    "df = pd.DataFrame(data, options_list, title)\n",
    "df.name = \"Time Per Epoch of Different Batch Sizes\"\n",
    "\n",
    "b_64_a = [0.7229, 0.7459, 0.7864, 0.8098, 0.8231, 0.8325, 0.8401, 0.8461, 0.8507, 0.8549, 0.8549-0.7229]\n",
    "b_128_a = [0.7139, 0.7333, 0.7413, 0.7595, 0.7832, 0.7989, 0.8092, 0.8177, 0.8240, 0.8304, 0.8304-0.7139]\n",
    "b_256_a = [0.6949, 0.7271, 0.7331, 0.7341, 0.7390, 0.7463, 0.7579, 0.7702, 0.7833, 0.7915, 0.7915-0.6949]\n",
    "options_list = [\"batch size = 64\", \"batch size = 128\", \"batch size = 256\"]\n",
    "b_64_a.append(mean(b_64_a))\n",
    "b_128_a.append(mean(b_128_a))\n",
    "b_256_a.append(mean(b_256_a))\n",
    "data_2 = [b_64_a, b_128_a, b_256_a] \n",
    "title_2 = list(range(1,11))\n",
    "title_2.append('increase of accuracy')\n",
    "title_2.append('average accuracy')\n",
    "df_2 = pd.DataFrame(data_2, options_list, title_2)\n",
    "df_2.name = \"Accuracy of Every Epoch of Different Batch Sizes\"\n",
    "\n",
    "print(df.name)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Every Epoch of Different Batch Sizes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>increase of accuracy</th>\n",
       "      <th>average accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>batch size = 64</th>\n",
       "      <td>0.7229</td>\n",
       "      <td>0.7459</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.8231</td>\n",
       "      <td>0.8325</td>\n",
       "      <td>0.8401</td>\n",
       "      <td>0.8461</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.8549</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.749491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch size = 128</th>\n",
       "      <td>0.7139</td>\n",
       "      <td>0.7333</td>\n",
       "      <td>0.7413</td>\n",
       "      <td>0.7595</td>\n",
       "      <td>0.7832</td>\n",
       "      <td>0.7989</td>\n",
       "      <td>0.8092</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.8304</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.720718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch size = 256</th>\n",
       "      <td>0.6949</td>\n",
       "      <td>0.7271</td>\n",
       "      <td>0.7331</td>\n",
       "      <td>0.7341</td>\n",
       "      <td>0.7390</td>\n",
       "      <td>0.7463</td>\n",
       "      <td>0.7579</td>\n",
       "      <td>0.7702</td>\n",
       "      <td>0.7833</td>\n",
       "      <td>0.7915</td>\n",
       "      <td>0.0966</td>\n",
       "      <td>0.688545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       1       2       3       4       5       6       7  \\\n",
       "batch size = 64   0.7229  0.7459  0.7864  0.8098  0.8231  0.8325  0.8401   \n",
       "batch size = 128  0.7139  0.7333  0.7413  0.7595  0.7832  0.7989  0.8092   \n",
       "batch size = 256  0.6949  0.7271  0.7331  0.7341  0.7390  0.7463  0.7579   \n",
       "\n",
       "                       8       9      10  increase of accuracy  \\\n",
       "batch size = 64   0.8461  0.8507  0.8549                0.1320   \n",
       "batch size = 128  0.8177  0.8240  0.8304                0.1165   \n",
       "batch size = 256  0.7702  0.7833  0.7915                0.0966   \n",
       "\n",
       "                  average accuracy  \n",
       "batch size = 64           0.749491  \n",
       "batch size = 128          0.720718  \n",
       "batch size = 256          0.688545  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_2.name)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As we increase the batch size from 64, 128, to 256, the speed of training will increase (in other words, the average training time per epoch will decrease), this is because we compute and update training parameters less often as the batch size increases.\n",
    "However, as we increase the batch size, the average accuracy and increase of accuracy will decrease, this is also because we adjust parameters less often.\n",
    "Tradeoff: compared to a larger batch size, the samller one will make training speed lower but accuracy higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another decision to make about the architecture of the NMT is the latent dimension size of the LSTM block. The LSTM block encodes semantics of a sentence to a vector of size equal to the latent dimension. This controls the modeling capacity of our system.\n",
    "\n",
    "**Deliverable 2** (5 points): Change the latent dimension from a 100 to a 10, and a 1000. How did it affect speed of training? Why? How did affect the accuracy of the system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Per Epoch of Different Latent Dimension\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>average time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>latent dimension = 10</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latent dimension = 100</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latent dimension = 1000</th>\n",
       "      <td>206</td>\n",
       "      <td>209</td>\n",
       "      <td>206</td>\n",
       "      <td>210</td>\n",
       "      <td>219</td>\n",
       "      <td>207</td>\n",
       "      <td>202</td>\n",
       "      <td>211</td>\n",
       "      <td>203</td>\n",
       "      <td>210</td>\n",
       "      <td>208.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           1    2    3    4    5    6    7    8    9   10  \\\n",
       "latent dimension = 10      5    4    3    4    4    4    3    3    3    3   \n",
       "latent dimension = 100     9    9    9   10    9    8    9    8    8    8   \n",
       "latent dimension = 1000  206  209  206  210  219  207  202  211  203  210   \n",
       "\n",
       "                         average time (s)  \n",
       "latent dimension = 10                 3.6  \n",
       "latent dimension = 100                8.7  \n",
       "latent dimension = 1000             208.3  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_100 = [9, 9, 9, 10, 9, 8, 9, 8, 8, 8]\n",
    "l_10 = [5, 4, 3, 4, 4, 4, 3, 3, 3, 3]\n",
    "l_1000 = [206, 209, 206, 210, 219, 207, 202, 211, 203, 210]\n",
    "options_list = [\"latent dimension = 10\", \"latent dimension = 100\", \"latent dimension = 1000\"]\n",
    "l_10.append(mean(l_10))\n",
    "l_100.append(mean(l_100))\n",
    "l_1000.append(mean(l_1000))\n",
    "data = [l_10, l_100, l_1000] \n",
    "title = list(range(1,11))\n",
    "title.append('average time (s)')\n",
    "df = pd.DataFrame(data, options_list, title)\n",
    "df.name = \"Time Per Epoch of Different Latent Dimension\"\n",
    "\n",
    "l_100_a = [0.7229, 0.7459, 0.7864, 0.8098, 0.8231, 0.8325, 0.8401, 0.8461, 0.8507, 0.8549]\n",
    "l_10_a = [0.6537, 0.7225, 0.7225, 0.7258, 0.7394, 0.7481, 0.7516, 0.7604, 0.7732, 0.7788]\n",
    "l_1000_a = [0.7225, 0.7842, 0.8086, 0.8332, 0.8499, 0.8653, 0.8780, 0.8898, 0.8999, 0.9084]\n",
    "options_list = [\"latent dimension = 10\", \"latent dimension = 100\", \"latent dimension = 1000\"]\n",
    "l_10_a.append(mean(l_10_a))\n",
    "l_100_a.append(mean(l_100_a))\n",
    "l_1000_a.append(mean(l_1000_a))\n",
    "data_2 = [l_10_a, l_100_a, l_1000_a] \n",
    "title_2 = list(range(1,11))\n",
    "title_2.append('average accuracy')\n",
    "df_2 = pd.DataFrame(data_2, options_list, title_2)\n",
    "df_2.name = \"Accuracy of Every Epoch of Different Batch Sizes\"\n",
    "\n",
    "print(df.name)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Every Epoch of Different Batch Sizes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>average accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>latent dimension = 10</th>\n",
       "      <td>0.6537</td>\n",
       "      <td>0.7225</td>\n",
       "      <td>0.7225</td>\n",
       "      <td>0.7258</td>\n",
       "      <td>0.7394</td>\n",
       "      <td>0.7481</td>\n",
       "      <td>0.7516</td>\n",
       "      <td>0.7604</td>\n",
       "      <td>0.7732</td>\n",
       "      <td>0.7788</td>\n",
       "      <td>0.73760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latent dimension = 100</th>\n",
       "      <td>0.7229</td>\n",
       "      <td>0.7459</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.8231</td>\n",
       "      <td>0.8325</td>\n",
       "      <td>0.8401</td>\n",
       "      <td>0.8461</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.8549</td>\n",
       "      <td>0.81124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latent dimension = 1000</th>\n",
       "      <td>0.7225</td>\n",
       "      <td>0.7842</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.8332</td>\n",
       "      <td>0.8499</td>\n",
       "      <td>0.8653</td>\n",
       "      <td>0.8780</td>\n",
       "      <td>0.8898</td>\n",
       "      <td>0.8999</td>\n",
       "      <td>0.9084</td>\n",
       "      <td>0.84398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              1       2       3       4       5       6  \\\n",
       "latent dimension = 10    0.6537  0.7225  0.7225  0.7258  0.7394  0.7481   \n",
       "latent dimension = 100   0.7229  0.7459  0.7864  0.8098  0.8231  0.8325   \n",
       "latent dimension = 1000  0.7225  0.7842  0.8086  0.8332  0.8499  0.8653   \n",
       "\n",
       "                              7       8       9      10  average accuracy  \n",
       "latent dimension = 10    0.7516  0.7604  0.7732  0.7788           0.73760  \n",
       "latent dimension = 100   0.8401  0.8461  0.8507  0.8549           0.81124  \n",
       "latent dimension = 1000  0.8780  0.8898  0.8999  0.9084           0.84398  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_2.name)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As the latent dimension increases from 10, 100, to 1000, the speed of training decreases (in other words, the training time per epoch increases), but the accuracy the accuracy of the system, this is because if we increase the latent dimension, there are more variables to the neural network, and we need to train more parameters during the training process. But the accuracy is higher when the latent dimension is larger since we keep more information by keeping a larger latent dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3** (5 points): One of the issues with the code above is that at demo time (for loop at the bottom of the code), the test is performed on training instances instead of validation instances. Adjust the code to perform the demo on the first 10 instances of the validation instances, and copy the cell's output below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The output is:\n",
    "\n",
    "Input sentence: I saw her home.\n",
    "Decoded sentence: Je  e  ais e.\n",
    "\n",
    "-\n",
    "Input sentence: I saw her home.\n",
    "Decoded sentence: Je  e  ais e.\n",
    "\n",
    "-\n",
    "Input sentence: I saw her swim.\n",
    "Decoded sentence: Je  e  ais e.\n",
    "\n",
    "-\n",
    "Input sentence: I saw him jump.\n",
    "Decoded sentence: Je  e  ais e.\n",
    "\n",
    "-\n",
    "Input sentence: I saw him once.\n",
    "Decoded sentence: Je  e  ais e.\n",
    "\n",
    "-\n",
    "Input sentence: I saw it first.\n",
    "Decoded sentence: Je  e  ais e.\n",
    "\n",
    "-\n",
    "Input sentence: I saw it on TV.\n",
    "Decoded sentence: Je  e  ais e.\n",
    "\n",
    "-\n",
    "Input sentence: I saw somebody.\n",
    "Decoded sentence: Je  e  ais e.\n",
    "\n",
    "-\n",
    "Input sentence: I saw somebody.\n",
    "Decoded sentence: Je  e  ais e.\n",
    "\n",
    "-\n",
    "Input sentence: I saw the cook.\n",
    "Decoded sentence: Je  e  ais e.\n",
    "\n",
    "The decoded sentences may vary due to randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 4** (5 points): In the following piece of code `decoder_dense = Dense(num_decoder_tokens, activation='softmax')` a dense layer is instantiated to map the output of the LSTM block to a prediction of the next character. Why is the output size of this dense layer set to `num_decoder_tokens`, which is the number of possible French output characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "print(num_decoder_tokens)\n",
    "print(len(target_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Because target sentences should be in French, therefore, the output characters should be French characters. There are 93 possible French characters as the output of our neural network, so we should set the output size of the dense layer to 93, which is just the number of decoder tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question** (10 points): During training, the model, besides being fed the target data (French translation) as in the normal supervised procedure, it is also fed the target data shifted by one position to the left. What is the purpose of this practice?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Answer**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By feeding the target data shifted by one position to the left, we can predict the next output by the previous context (or in other words, given the predicted context data). Because languages are highly dependent on the context, it could be better if we not only consider the current character, but also consider the previous context when doing translation, so this shifting of data could very possibly improve our translation, making it more fluent and grammatical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 1 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 1.7467 - accuracy: 0.6949 - val_loss: 1.2967 - val_accuracy: 0.6901\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 8s 981us/step - loss: 1.1033 - accuracy: 0.7271 - val_loss: 1.2333 - val_accuracy: 0.6959\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 8s 981us/step - loss: 1.0366 - accuracy: 0.7331 - val_loss: 1.1389 - val_accuracy: 0.6962\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 8s 976us/step - loss: 0.9847 - accuracy: 0.7341 - val_loss: 1.0803 - val_accuracy: 0.7005\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 8s 975us/step - loss: 0.9443 - accuracy: 0.7390 - val_loss: 1.0387 - val_accuracy: 0.6977\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 8s 984us/step - loss: 0.9060 - accuracy: 0.7463 - val_loss: 1.0047 - val_accuracy: 0.7112\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 8s 978us/step - loss: 0.8751 - accuracy: 0.7579 - val_loss: 0.9722 - val_accuracy: 0.7258\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 8s 976us/step - loss: 0.8373 - accuracy: 0.7702 - val_loss: 0.9348 - val_accuracy: 0.7431\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 8s 980us/step - loss: 0.7994 - accuracy: 0.7833 - val_loss: 0.9000 - val_accuracy: 0.7457\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 8s 976us/step - loss: 0.7619 - accuracy: 0.7915 - val_loss: 0.8727 - val_accuracy: 0.7628\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Tous e                                                      \n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Tous e                                                      \n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Tous e                                                      \n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Tous e                                                      \n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Ares e                                                      \n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arete                                                       \n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arete                                                       \n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arete                                                       \n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Arete                                                       \n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Arete                                                       \n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Arete                                                       \n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Arete                                                       \n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je se s ai                                                  \n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Il ais e  ai                                                \n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je se te  ai                                                \n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je se te  ai                                                \n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: Je se s ai                                                  \n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Ate te                                                      \n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Ate te                                                      \n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Ates e                                                      \n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Ates e                                                      \n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Ates e                                                      \n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Ates e                                                      \n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Arete                                                       \n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Ates e                                                      \n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Ates e                                                      \n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Arete                                                       \n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Arete                                                       \n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Arete                                                       \n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je se s ais  a                                              \n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je se s ais  a                                              \n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je se s ai                                                  \n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je se s ais  a                                              \n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je se s ais  a                                              \n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: Je se s ai                                                  \n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: Je se te  ai                                                \n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Je se s ais  a                                              \n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je se s ais  a                                              \n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je se s ais  a                                              \n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Ate e                                                       \n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Ate e                                                       \n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Ate e                                                       \n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: Ales e                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Tous e  ai                                                  \n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Tous e  ai                                                  \n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Tous e  ai                                                  \n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Tous e  ai                                                  \n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Arete                                                       \n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Ares e                                                      \n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Ares e                                                      \n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Ales e                                                      \n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Ates e                                                      \n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Ates e                                                      \n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Ates e                                                      \n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Ates e                                                      \n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Ates e                                                      \n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Ares e                                                      \n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Ares e                                                      \n"
     ]
    }
   ],
   "source": [
    "batch_size = 256  # Batch size for training, change from 64 to 128 to 256.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "latent_dim = 100  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 2 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 206s 26ms/step - loss: 1.2202 - accuracy: 0.7225 - val_loss: 1.0714 - val_accuracy: 0.7065\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 209s 26ms/step - loss: 0.8203 - accuracy: 0.7842 - val_loss: 0.7749 - val_accuracy: 0.7784\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 206s 26ms/step - loss: 0.6708 - accuracy: 0.8086 - val_loss: 0.7021 - val_accuracy: 0.7971\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 210s 26ms/step - loss: 0.5693 - accuracy: 0.8332 - val_loss: 0.6212 - val_accuracy: 0.8163\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 219s 27ms/step - loss: 0.5075 - accuracy: 0.8499 - val_loss: 0.5704 - val_accuracy: 0.8311\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 207s 26ms/step - loss: 0.4517 - accuracy: 0.8653 - val_loss: 0.5212 - val_accuracy: 0.8425\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 202s 25ms/step - loss: 0.4059 - accuracy: 0.8780 - val_loss: 0.4914 - val_accuracy: 0.8524\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 211s 26ms/step - loss: 0.3650 - accuracy: 0.8898 - val_loss: 0.4609 - val_accuracy: 0.8637\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 203s 25ms/step - loss: 0.3295 - accuracy: 0.8999 - val_loss: 0.4618 - val_accuracy: 0.8633\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 210s 26ms/step - loss: 0.2985 - accuracy: 0.9084 - val_loss: 0.4454 - val_accuracy: 0.8704\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Comment va sentir ?\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Comment va sentir ?\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui est cela ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Attends une seconde !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Vourris l'aiser.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: ArrÃªte de te la sourir !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: ArrÃªte de te la sourir !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: ArrÃªte de te la sourir !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends une seconde !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends une seconde !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Comment va sentir ?\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Comment va sentir ?\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je me suis rendue.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Je me suis rendue.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je me suis rendue.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je me suis rendue.\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: Je me suis rendue.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: C'est le mien.\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Venez vite !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Venez vite !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Vourris l'aiser.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Vourris l'aiser.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Vourris l'aiser.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Vourris l'aiser.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Allez chercher votre lache.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Allez vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Soyez prudent !\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Soyez prudent !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Apportez de le voirurer !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Apportez de le voirurer !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je me suis rendue.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je me suis rendue.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je me suis rendue.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: J'ai pris ma chien.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: J'ai pris ma chien.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: J'aime les filles.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: Je me suis rendue.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Je suis trapaille.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis trapaille.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis trapaille.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Laisse-moi partir.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'a parlÃ©.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'a parlÃ©.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'a parlÃ©.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'a parlÃ©.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'a parlÃ©.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'a parlÃ©.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'a parlÃ©.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'a parlÃ©.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'a parlÃ©.\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Allez chercher votre lache.\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Allez chercher votre lache.\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Allez chercher votre lache.\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: C'est le mien.\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: Nous sommes en train de manger.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous sommes en train de manger.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous sommes en train de manger.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous sommes en train de manger.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous sommes en train de manger.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Ãtes-vous sÃ»res ?\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Attends une seconde !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez prudent !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez prudent !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez prudent !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Soyez prudent !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Soyez prudent !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez confiantes !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Soyez prudent !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Prends une boisson !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Prends une boisson !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Prends une boisson !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Prends une boisson !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Viens vite !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Viens vite !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "latent_dim = 1000  # Latent dimensionality of the encoding space, change from 10 to 100 to 1000.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 3 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 1.8094 - accuracy: 0.6979 - val_loss: 1.2700 - val_accuracy: 0.6929\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 1.1038 - accuracy: 0.7295 - val_loss: 1.1775 - val_accuracy: 0.6959\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 1.0395 - accuracy: 0.7328 - val_loss: 1.1190 - val_accuracy: 0.6959\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.9848 - accuracy: 0.7333 - val_loss: 1.0745 - val_accuracy: 0.6999\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.9423 - accuracy: 0.7368 - val_loss: 1.0491 - val_accuracy: 0.7030\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.9098 - accuracy: 0.7444 - val_loss: 1.0092 - val_accuracy: 0.7123\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 8s 975us/step - loss: 0.8779 - accuracy: 0.7558 - val_loss: 0.9805 - val_accuracy: 0.7237\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 8s 973us/step - loss: 0.8416 - accuracy: 0.7692 - val_loss: 0.9365 - val_accuracy: 0.7410\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 8s 988us/step - loss: 0.8058 - accuracy: 0.7814 - val_loss: 0.8957 - val_accuracy: 0.7559\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 8s 972us/step - loss: 0.7682 - accuracy: 0.7913 - val_loss: 0.8914 - val_accuracy: 0.7513\n",
      "-\n",
      "Input sentence: I saw her home.\n",
      "Decoded sentence: Je  e  ais e.\n",
      "\n",
      "-\n",
      "Input sentence: I saw her home.\n",
      "Decoded sentence: Je  e  ais e.\n",
      "\n",
      "-\n",
      "Input sentence: I saw her swim.\n",
      "Decoded sentence: Je  e  ais e.\n",
      "\n",
      "-\n",
      "Input sentence: I saw him jump.\n",
      "Decoded sentence: Je  e  ais e.\n",
      "\n",
      "-\n",
      "Input sentence: I saw him once.\n",
      "Decoded sentence: Je  e  ais e.\n",
      "\n",
      "-\n",
      "Input sentence: I saw it first.\n",
      "Decoded sentence: Je  e  ais e.\n",
      "\n",
      "-\n",
      "Input sentence: I saw it on TV.\n",
      "Decoded sentence: Je  e  ais e.\n",
      "\n",
      "-\n",
      "Input sentence: I saw somebody.\n",
      "Decoded sentence: Je  e  ais e.\n",
      "\n",
      "-\n",
      "Input sentence: I saw somebody.\n",
      "Decoded sentence: Je  e  ais e.\n",
      "\n",
      "-\n",
      "Input sentence: I saw the cook.\n",
      "Decoded sentence: Je  e  ais e.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "latent_dim = 100  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "validation = int(len(encoder_input_data)*0.8)\n",
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[validation + seq_index:validation + seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[validation + seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
